import streamlit as st
import pandas as pd
import io
from Casos_Prueba_IA import setup_gemini, generar_casos_prueba, DEFAULT_MODEL_NAME, DEFAULT_PROMPT

st.set_page_config(
    page_title="Generador de Casos de Prueba IA",
    page_icon="üß™",
    layout="wide"
)

st.markdown("""
<style>
    .stButton>button {
        width: 100%;
        background-color: #4CAF50;
        color: white;
    }
</style>
""", unsafe_allow_html=True)

# ==============================
# PROMPT PARA EL CHAT CON CONTEXTO
# ==============================
CHAT_PROMPT_CON_CONTEXTO = """
Eres un ingeniero QA senior, experto en certificaci√≥n, pruebas t√©cnicas y dise√±o
estructurado de casos de prueba.

**CONTEXTO ACTUAL:**
El usuario est√° trabajando con la siguiente Historia de Usuario (HU):

--- INICIO DE HU ---
{contexto_hu}
--- FIN DE HU ---

Tu objetivo es ayudar al usuario respondiendo preguntas sobre:
- Esta Historia de Usuario espec√≠fica
- Casos de prueba relacionados con esta HU
- Mejoras o sugerencias para esta HU
- An√°lisis de criterios de aceptaci√≥n
- Estrategias de pruebas (funcionales, negativas, l√≠mite, etc.)

Responde de manera conversacional, clara y profesional. Si el usuario pregunta algo
relacionado con la HU, usa el contexto proporcionado. Si no hay contexto, indica
que primero debe cargar una HU.

**Pregunta del usuario:**
{mensaje_usuario}
"""

CHAT_PROMPT_SIN_CONTEXTO = """
Eres un ingeniero QA senior, experto en certificaci√≥n, pruebas t√©cnicas y dise√±o
estructurado de casos de prueba.

Tu objetivo es ayudar al usuario respondiendo preguntas sobre:
- Historias de Usuario (HU)
- Casos de prueba y metodolog√≠as de testing
- Mejores pr√°cticas de QA
- An√°lisis de criterios de aceptaci√≥n
- Estrategias de pruebas (funcionales, negativas, l√≠mite, etc.)

Responde de manera conversacional, clara y profesional.

**Nota:** Actualmente no hay ninguna Historia de Usuario cargada. Si el usuario quiere
analizar una HU espec√≠fica, debe primero cargarla usando las pesta√±as de arriba.

**Pregunta del usuario:**
{mensaje_usuario}
"""

def main():
    st.title("üß™ Generador de Casos de Prueba con IA")
    st.markdown("Sube tus Historias de Usuario (HU) o p√©galas directamente para generar casos de prueba exhaustivos.")

    # --- SIDEBAR: Configuraci√≥n ---
    with st.sidebar:
        st.header("‚öôÔ∏è Configuraci√≥n")
        
        api_key = st.text_input(
            "Gemini API Key", 
            value="AIzaSyCnsRfsOnX8RjD3a_tDgaT5T7yLtBiEwJM",
            type="password", 
            help="Ingresa tu API Key de Google Gemini."
        )
        
        model_name = st.selectbox(
            "Modelo", 
            options=["gemini-2.5-flash", "gemini-pro"],
            index=0
        )
        
        temperature = st.slider(
            "Creatividad (Temperatura)", 
            min_value=0.0, 
            max_value=1.0, 
            value=0.4,
            step=0.1
        )

        st.info("Nota: La API Key no se guarda, solo se usa para esta sesi√≥n.")
        
        # Mostrar contexto actual
        st.markdown("---")
        st.subheader("üìÑ Contexto del Chat")
        if "contexto_hu" in st.session_state and st.session_state.contexto_hu:
            with st.expander("Ver HU Actual", expanded=False):
                st.text_area(
                    "Historia de Usuario en memoria:",
                    value=st.session_state.contexto_hu[:500] + "..." if len(st.session_state.contexto_hu) > 500 else st.session_state.contexto_hu,
                    height=200,
                    disabled=True
                )
            if st.button("üóëÔ∏è Limpiar Contexto"):
                st.session_state.contexto_hu = ""
                st.session_state.messages = []
                st.rerun()
        else:
            st.warning("‚ö†Ô∏è No hay HU cargada. El chat responder√° de forma general.")
        
        with st.expander("üìù Editar Prompt del Sistema (Para CSV)"):
            st.warning("‚ö† Aseg√∫rate de mantener `{hu_texto}` donde quieras que vaya la HU.")
            custom_prompt_input = st.text_area(
                "Prompt del Sistema",
                value=DEFAULT_PROMPT,
                height=400
            )

    # Inicializar contexto
    if "contexto_hu" not in st.session_state:
        st.session_state.contexto_hu = ""

    # --- √ÅREA PRINCIPAL ---
    tab_archivos, tab_texto = st.tabs(["üìÇ Subir Archivos", "üìù Pegar Texto"])
    
    hus_para_procesar = []

    with tab_archivos:
        uploaded_files = st.file_uploader(
            "Arrastra tus archivos .txt aqu√≠", 
            type=["txt"], 
            accept_multiple_files=True
        )
        if uploaded_files:
            st.success(f"{len(uploaded_files)} archivos cargados.")
            for uploaded_file in uploaded_files:
                string_data = uploaded_file.getvalue().decode("utf-8")
                hus_para_procesar.append((uploaded_file.name, string_data))

    with tab_texto:
        texto_manual = st.text_area(
            "Pega aqu√≠ el contenido de tu Historia de Usuario", 
            height=300,
            placeholder="Como usuario quiero..."
        )
        if texto_manual.strip():
            hus_para_procesar.append(("Texto Manual", texto_manual))

    # --- BOT√ìN DE GENERAR ---
    if st.button("üöÄ Generar Casos de Prueba", type="primary"):
        if not api_key:
            st.error("‚ùå Por favor ingresa tu API Key en la barra lateral.")
            return

        if not hus_para_procesar:
            st.warning("‚ö† No hay HUs para procesar. Sube archivos o pega texto.")
            return

        # SETUP DEL MODELO
        try:
            model = setup_gemini(api_key, model_name, temperature)
            st.session_state.model = model
        except Exception as e:
            st.error(f"Error al configurar Gemini: {e}")
            return

        # GUARDAR CONTEXTO (la primera o √∫ltima HU)
        # Opci√≥n 1: Guardar la primera HU
        st.session_state.contexto_hu = hus_para_procesar[0][1]
        
        # Opci√≥n 2: Guardar todas concatenadas (si son pocas)
        # st.session_state.contexto_hu = "\n\n---\n\n".join([contenido for _, contenido in hus_para_procesar])

        # PROCESAMIENTO
        all_cases = []
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        total_hus = len(hus_para_procesar)
        
        for i, (nombre, contenido) in enumerate(hus_para_procesar):
            status_text.text(f"Procesando: {nombre}...")
            try:
                casos = generar_casos_prueba(model, contenido, custom_prompt=custom_prompt_input)
                
                for c in casos:
                    c["archivo_hu"] = nombre
                
                all_cases.extend(casos)
                
            except Exception as e:
                st.error(f"Error procesando {nombre}: {e}")
            
            progress_bar.progress((i + 1) / total_hus)

        progress_bar.empty()
        status_text.empty()

        if all_cases:
            st.success("‚úÖ ¬°Generaci√≥n completada!")
            st.info(f"üí° El chat ahora tiene contexto de la HU. Puedes hacerle preguntas sobre ella.")
            
            df = pd.DataFrame(all_cases)
            
            cols_order = [
                "archivo_hu", 
                "id_caso", 
                "tipo_prueba", 
                "prioridad",
                "Automatizar",
                "descripcion", 
                "precondiciones", 
                "pasos", 
                "resultado_esperado", 
                "criterio"
            ]
            cols_final = [c for c in cols_order if c in df.columns]
            df = df[cols_final]

            st.subheader("üìã Resultados")
            st.dataframe(df, use_container_width=True)

            csv_buffer = io.StringIO()
            df.to_csv(csv_buffer, index=False, sep=';', encoding='utf-8-sig')
            csv_data = csv_buffer.getvalue()

            st.download_button(
                label="üì• Descargar CSV",
                data=csv_data,
                file_name="casos_prueba_generados.csv",
                mime="text/csv"
            )
        else:
            st.warning("No se generaron casos de prueba. Revisa el log de errores.")

    # --- SECCI√ìN DE CHAT ---
    st.markdown("---")
    st.header("üí¨ Chat con IA sobre Testing")
    
    # Indicador de contexto
    if st.session_state.contexto_hu:
        st.success("‚úÖ Chat contextualizado con HU actual")
    else:
        st.info("‚ÑπÔ∏è Chat en modo general (sin HU cargada)")

    # Inicializar historial
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Mostrar historial
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Entrada de chat
    if prompt := st.chat_input("Preg√∫ntame sobre la HU, casos de prueba, o metodolog√≠as QA..."):
        if not api_key:
            st.error("‚ùå Por favor ingresa tu API Key en la barra lateral para chatear.")
            return

        # Agregar mensaje del usuario
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # Configurar modelo si no existe
        if "model" not in st.session_state:
            try:
                st.session_state.model = setup_gemini(api_key, model_name, temperature)
            except Exception as e:
                st.error(f"Error al configurar Gemini para chat: {e}")
                return

        # SELECCIONAR PROMPT SEG√öN CONTEXTO
        if st.session_state.contexto_hu:
            # Hay contexto ‚Üí Usar prompt con HU
            chat_prompt = CHAT_PROMPT_CON_CONTEXTO.format(
                contexto_hu=st.session_state.contexto_hu,
                mensaje_usuario=prompt
            )
        else:
            # No hay contexto ‚Üí Usar prompt general
            chat_prompt = CHAT_PROMPT_SIN_CONTEXTO.format(
                mensaje_usuario=prompt
            )

        # Generar respuesta
        try:
            with st.spinner("Pensando..."):
                response = st.session_state.model.generate_content(chat_prompt)
                respuesta_texto = response.text

            # Agregar respuesta al historial
            st.session_state.messages.append({"role": "assistant", "content": respuesta_texto})
            with st.chat_message("assistant"):
                st.markdown(respuesta_texto)
        except Exception as e:
            st.error(f"Error en la respuesta del chat: {e}")

if __name__ == "__main__":
    main()